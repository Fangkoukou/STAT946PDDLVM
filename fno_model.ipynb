{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cb268e-579f-4946-8e50-9710e7dec23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from pfc_data_generate import generate_data\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.init()\n",
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5f241a-a97d-482b-ba35-85c118ce1cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for printing and saving\n",
    "def print_epoch_summary(epoch, epochs, data_loss, physics_loss, time_used):\n",
    "    \"\"\"Prints formatted training progress summary for each epoch.\n",
    "    \n",
    "    Args:\n",
    "        epoch: Current epoch index (0-based)\n",
    "        epochs: Total number of epochs\n",
    "        data_loss: Supervised loss from ground truth data\n",
    "        physics_loss: Physics-informed regularization loss\n",
    "        time_used: Time taken for the epoch in seconds\n",
    "    \"\"\"\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] | \"\n",
    "          f\"Data Loss: {data_loss:.4e} | \"\n",
    "          f\"Physics Loss: {physics_loss:.4e} | \"\n",
    "          f\"Time: {time_used:.1f}s\")\n",
    "\n",
    "def save_best_model(model, loss, save_dir=\"fno_model\"):\n",
    "    \"\"\"Saves model weights when validation loss improves.\n",
    "    \n",
    "    Args:\n",
    "        model: Model instance to save\n",
    "        loss: Validation loss value for comparison\n",
    "        save_dir: Directory to save checkpoints\n",
    "    \n",
    "    Creates directory if needed and preserves best performing weights.\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    path = os.path.join(save_dir, \"best_model.pth\")\n",
    "    torch.save(model.state_dict(), path)\n",
    "    print(f\"Saved best model with loss {loss:.4e} to {path}\")\n",
    "\n",
    "# Data preparation pipeline\n",
    "def prepare_data(gamma, grid_dim, L, dt, T, Nskip):\n",
    "    \"\"\"Generates and processes training data for PDE system.\n",
    "    \n",
    "    Args:\n",
    "        gamma: System parameter (e.g., viscosity)\n",
    "        grid_dim: Number of spatial grid points\n",
    "        L: Domain size [L × L]\n",
    "        dt: Time step size\n",
    "        T: Total simulation time\n",
    "        Nskip: Number of steps between saved snapshots\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (input_sequence, target_sequence, k2, dealias_mask, dt)\n",
    "        - input/target: Consecutive time steps [N_samples, 1, grid_dim, grid_dim]\n",
    "        - k2: Precomputed Fourier space Laplacian\n",
    "        - dealias: Spectral dealiasing filter\n",
    "    \"\"\"\n",
    "    # Generate raw simulation data using external solver\n",
    "    data, k2, dealias, t = generate_data(gamma, grid_dim, L, dt, T, Nskip)\n",
    "    \n",
    "    # Convert to PyTorch tensors and add channel dimension\n",
    "    data = torch.tensor(data).float().unsqueeze(1).to(device)  # Shape: [N, 1, H, W]\n",
    "    \n",
    "    # Prepare physical parameters for GPU computation\n",
    "    k2 = torch.tensor(k2).float().to(device)       # Fourier-space Laplacian\n",
    "    dealias = torch.tensor(dealias).float().to(device)  # Dealiasing mask\n",
    "    \n",
    "    # Create input-target pairs: predict next time step\n",
    "    return data[:-1], data[1:], k2, dealias, dt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372d4c7a-f54b-49e6-ba79-f58c81c4c136",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectralConv2d(nn.Module):\n",
    "    \"\"\"Spectral convolution layer using Fourier transforms.\n",
    "    \n",
    "    Operates in frequency domain to capture global spatial relationships efficiently.\n",
    "    Uses truncated Fourier series to maintain parameter efficiency.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, modes1, modes2):\n",
    "        super().__init__()\n",
    "        self.modes1 = modes1  # Number of Fourier modes to keep in spatial dim 1\n",
    "        self.modes2 = modes2  # Number of Fourier modes to keep in spatial dim 2\n",
    "        \n",
    "        # Scale factor for weight initialization (maintain variance)\n",
    "        self.scale = 1 / (in_channels * out_channels)\n",
    "        \n",
    "        # Learnable complex-valued weights for frequency domain operations\n",
    "        self.weights1 = nn.Parameter(\n",
    "            self.scale * torch.rand(in_channels, out_channels, \n",
    "                                  modes1, modes2, dtype=torch.cfloat))\n",
    "        self.weights2 = nn.Parameter(\n",
    "            self.scale * torch.rand(in_channels, out_channels, \n",
    "                                  modes1, modes2, dtype=torch.cfloat))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input shape: [batch, channels, height, width]\n",
    "        B, _, H, W = x.shape\n",
    "        \n",
    "        # Transform to Fourier space (real FFT for real-valued inputs)\n",
    "        x_ft = torch.fft.rfft2(x)  # Complex tensor shape: [B, C, H, W//2+1]\n",
    "        \n",
    "        # Initialize output Fourier tensor\n",
    "        out_ft = torch.zeros(B, self.weights1.shape[1], H, W//2 + 1, \n",
    "                           dtype=torch.cfloat, device=x.device)\n",
    "        \n",
    "        # Multiply relevant Fourier modes (lower frequencies)\n",
    "        out_ft[:, :, :self.modes1, :self.modes2] = torch.einsum(\n",
    "            \"bixy,ioxy->boxy\", x_ft[:, :, :self.modes1, :self.modes2], self.weights1)\n",
    "        \n",
    "        # Multiply higher frequencies in spatial dimension 1\n",
    "        out_ft[:, :, -self.modes1:, :self.modes2] = torch.einsum(\n",
    "            \"bixy,ioxy->boxy\", x_ft[:, :, -self.modes1:, :self.modes2], self.weights2)\n",
    "\n",
    "        # Return to spatial domain while preserving original dimensions\n",
    "        return torch.fft.irfft2(out_ft, s=(H, W))\n",
    "\n",
    "class FNO2d(nn.Module):\n",
    "    \"\"\"Fourier Neural Operator architecture for 2D PDE systems.\n",
    "    \n",
    "    Combines spectral convolutions with standard CNNs to model both \n",
    "    global and local spatial dependencies efficiently.\n",
    "    \"\"\"\n",
    "    def __init__(self, modes1=24, modes2=24, width=64):\n",
    "        super().__init__()\n",
    "        self.modes1 = modes1  # Fourier mode truncation in first spatial dimension\n",
    "        self.modes2 = modes2  # Fourier mode truncation in second spatial dimension\n",
    "        self.width = width    # Channel dimension size throughout network\n",
    "        \n",
    "        # Initial feature embedding (lifts input to higher dimension)\n",
    "        self.fc0 = nn.Linear(1, self.width)  # Input: [B, H, W, 1]\n",
    "        \n",
    "        # Fourier convolution layers with corresponding 1x1 convolutions\n",
    "        self.conv0 = SpectralConv2d(width, width, modes1, modes2)\n",
    "        self.conv1 = SpectralConv2d(width, width, modes1, modes2)\n",
    "        self.conv2 = SpectralConv2d(width, width, modes1, modes2)\n",
    "        self.conv3 = SpectralConv2d(width, width, modes1, modes2)\n",
    "        \n",
    "        # Pointwise convolutions for residual connections\n",
    "        self.w0 = nn.Conv2d(width, width, 1)  # 1x1 kernel for channel mixing\n",
    "        self.w1 = nn.Conv2d(width, width, 1)\n",
    "        self.w2 = nn.Conv2d(width, width, 1)\n",
    "        self.w3 = nn.Conv2d(width, width, 1)\n",
    "\n",
    "        # Final projection layers\n",
    "        self.fc1 = nn.Linear(width, 256)  # High-dim intermediate projection\n",
    "        self.fc2 = nn.Linear(256, 1)     # Output projection to physical space\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input shape: [batch, 1, height, width]\n",
    "        \n",
    "        # Dimensionality transformations for linear layer processing\n",
    "        x = x.permute(0, 2, 3, 1)  # [B, H, W, 1]\n",
    "        x = self.fc0(x)             # Lift to [B, H, W, width]\n",
    "        x = x.permute(0, 3, 1, 2)  # [B, width, H, W]\n",
    "        \n",
    "        # Sequence of Fourier layers with residual connections\n",
    "        for conv, w in zip([self.conv0, self.conv1, self.conv2, self.conv3],\n",
    "                          [self.w0, self.w1, self.w2, self.w3]):\n",
    "            x1 = conv(x)  # Spectral convolution (global processing)\n",
    "            x2 = w(x)    # Pointwise convolution (local processing)\n",
    "            x = F.silu(x1 + x2)  # Residual connection with activation\n",
    "        \n",
    "        # Final projections to output space\n",
    "        x = x.permute(0, 2, 3, 1)  # [B, H, W, width]\n",
    "        x = self.fc1(x)            # [B, H, W, 256]\n",
    "        x = F.silu(x)              # Non-linear activation\n",
    "        x = self.fc2(x)            # [B, H, W, 1]\n",
    "        return x.permute(0, 3, 1, 2)  # Return to [B, 1, H, W] shape\n",
    "\n",
    "def load_model_weights(model, weight_path):\n",
    "    \"\"\"\n",
    "    Loads model weights from a .pth file and sets the model to eval mode.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The model to load weights into.\n",
    "        weight_path (str): Path to the saved weights.\n",
    "    \n",
    "    Returns:\n",
    "        nn.Module: The model with loaded weights.\n",
    "    \"\"\"\n",
    "    state_dict = torch.load(weight_path, map_location=device)\n",
    "    # Remove 'module.' prefix if present\n",
    "    state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9350d0e9-60aa-4f9d-84f9-44706005e2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Noperator_func(u, k2, dealias):\n",
    "    \"\"\"\n",
    "    Computes the nonlinear term in Fourier space for the modified Kuramoto–Sivashinsky equation.\n",
    "    \"\"\"\n",
    "    return -(k2 * torch.fft.fft2(u**3)) * dealias\n",
    "\n",
    "def compute_physics_loss(u_prev, u_pred, dt, k2, dealias, lineardenominator_hat):\n",
    "    \"\"\"\n",
    "    Computes loss based on the discretized physics evolution.\n",
    "    \"\"\"\n",
    "    u_hat = (torch.fft.fft2(u_prev) + dt * Noperator_func(u_prev, k2, dealias)) * lineardenominator_hat\n",
    "    u_hat = torch.fft.ifft2(u_hat).real\n",
    "    return F.mse_loss(u_hat, u_pred)\n",
    "\n",
    "def train_fno(\n",
    "    gamma=0.25, grid_dim=64, L=16*np.pi, dt=0.1, T=1200,\n",
    "    Nskip=1, epochs=50, batch_size=24, modes=24, width=64\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains a Fourier Neural Operator on the nonlinear PDE data with optional physics-informed loss.\n",
    "    \"\"\"\n",
    "\n",
    "    # === Step 1: Prepare training data ===\n",
    "    inputs, targets, k2, dealias, dt = prepare_data(gamma, grid_dim, L, dt, T, Nskip)\n",
    "    dataset = TensorDataset(inputs, targets)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Compute linear operator used for physics update (in Fourier domain)\n",
    "    L_operator = -k2 * (k2**2 - 2*k2 + 1 - gamma)\n",
    "    lineardenominator_hat = 1 / (1 - dt * L_operator)\n",
    "\n",
    "    # === Step 2: Initialize model and optimizer ===\n",
    "    model = FNO2d(modes1=modes, modes2=modes, width=width).to(device)\n",
    "    model.train()\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=80)\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    # === Step 3: Training loop ===\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        data_loss = 0.0\n",
    "        physics_loss = 0.0\n",
    "        start_time = time.time()\n",
    "\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            # --- Forward and compute losses ---\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(x)\n",
    "\n",
    "            # Supervised MSE loss\n",
    "            loss_data = F.mse_loss(pred, y)\n",
    "\n",
    "            # Physics-based consistency loss\n",
    "            loss_physics = compute_physics_loss(\n",
    "                x.squeeze(1), pred.squeeze(1), dt, k2, dealias, lineardenominator_hat\n",
    "            )\n",
    "\n",
    "            # Combine losses (add physics loss with small weight if desired)\n",
    "            loss = loss_data  # + 0.1 * loss_physics\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Track losses\n",
    "            epoch_loss += loss.item()\n",
    "            data_loss += loss_data.item()\n",
    "            physics_loss += loss_physics.item()\n",
    "\n",
    "        # Learning rate scheduling based on validation performance\n",
    "        avg_loss = epoch_loss / len(loader)\n",
    "        scheduler.step(avg_loss)\n",
    "\n",
    "        # Save model if it's the best seen so far\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            save_best_model(model, best_loss)\n",
    "\n",
    "        # Epoch summary\n",
    "        print_epoch_summary(\n",
    "            epoch, epochs,\n",
    "            data_loss / len(loader),\n",
    "            physics_loss / len(loader),\n",
    "            time.time() - start_time\n",
    "        )\n",
    "\n",
    "    return model, inputs, targets, loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02463b8-7f47-4ae3-8a7a-dc6c972860ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model, inputs, targets, loader = train_fno(epochs=10, batch_size=80, modes=16, width=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0515dde4-4d16-455b-8ced-f682399df8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoregressive_prediction(model, initial_condition, steps=10):\n",
    "    \"\"\"\n",
    "    Generates predictions for multiple steps ahead by feeding model outputs as new inputs.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): Trained FNO model.\n",
    "        initial_condition (ndarray or tensor): Starting input with shape [B, H, W].\n",
    "        steps (int): Number of autoregressive steps to take.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: Stacked predictions over the sequence, shape [steps, B, 1, H, W].\n",
    "    \"\"\"\n",
    "    current_state = torch.tensor(initial_condition).float().unsqueeze(1).to(device)  # [B, 1, H, W]\n",
    "    predictions = []\n",
    "\n",
    "    for _ in range(steps):\n",
    "        with torch.no_grad():\n",
    "            next_step = model(current_state)\n",
    "        predictions.append(next_step)\n",
    "        current_state = next_step  # Use output as input for next step\n",
    "\n",
    "    return torch.concatenate(predictions, dim=0)\n",
    "\n",
    "def efficient_nth_step_prediction(model, initial_condition, n_steps):\n",
    "    \"\"\"\n",
    "    Predicts the state after n steps using only the last output (memory-efficient).\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): Trained model.\n",
    "        initial_condition (ndarray or tensor): Starting condition, shape [B, H, W].\n",
    "        n_steps (int): Number of steps to evolve the system.\n",
    "\n",
    "    Returns:\n",
    "        ndarray: The state after n steps, shape [H, W].\n",
    "    \"\"\"\n",
    "    current_state = torch.tensor(initial_condition).float().unsqueeze(1).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(n_steps):\n",
    "            current_state = model(current_state)\n",
    "\n",
    "    return current_state.squeeze(1).cpu().numpy()[0]  # Return only the first sample\n",
    "\n",
    "def compute_mse_per_sample(tensor1, tensor2):\n",
    "    \"\"\"\n",
    "    Computes per-sample mean squared error between two tensors.\n",
    "\n",
    "    Args:\n",
    "        tensor1, tensor2 (Tensor): Shape [B, C, H, W].\n",
    "\n",
    "    Returns:\n",
    "        Tensor: Shape [B], MSE for each sample in batch.\n",
    "    \"\"\"\n",
    "    squared_diff = (tensor1 - tensor2)**2\n",
    "    mse_per_sample = squared_diff.mean(dim=(1, 2, 3))  # Mean over C, H, W\n",
    "    return mse_per_sample\n",
    "\n",
    "def plot_mse_progression(mse_tensor, dt=None, save_path=None):\n",
    "    \"\"\"Visualizes MSE development over prediction steps.\n",
    "    \n",
    "    Args:\n",
    "        mse_tensor: Tensor containing MSE values per step\n",
    "        dt: Time step size for physical time axis labeling\n",
    "        save_path: Optional path to save figure\n",
    "    \n",
    "    Converts tensor to numpy, creates time axis if dt provided,\n",
    "    and displays/saves plot with formatting.\n",
    "    \"\"\"\n",
    "    mse_values = mse_tensor.detach().cpu().numpy()\n",
    "    x_values = dt * np.arange(len(mse_values)) if dt else np.arange(len(mse_values))\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(x_values, mse_values, color='blue', linewidth=1)\n",
    "    plt.xlabel('Physical Time' if dt else 'Prediction Step', fontsize=12)\n",
    "    plt.ylabel('Mean Squared Error', fontsize=12)\n",
    "    plt.title(\"Temporal Error Progression\", fontsize=14)\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, bbox_inches='tight', dpi=300)\n",
    "        print(f\"Error plot saved to {save_path}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce078a5-cbd5-43bb-8cb7-4c60c4017649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model for inference\n",
    "test_model = FNO2d(modes1=16, modes2=16, width=32).to(device)\n",
    "test_model = load_model_weights(test_model, \"fno_model/best_model.pth\")\n",
    "test_model.eval()\n",
    "\n",
    "# Set up prediction tensor\n",
    "initial_data = inputs[0]  # Just to inspect or use as seed\n",
    "num_frames = len(inputs)\n",
    "preds = torch.empty_like(inputs)  # [N, 1, H, W]\n",
    "\n",
    "# Predict frame-by-frame\n",
    "for i in range(num_frames):\n",
    "    frame = inputs[i].unsqueeze(0).to(device)  # [1, 1, H, W]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = test_model(frame)\n",
    "    \n",
    "    preds[i] = output.squeeze(0).cpu()  # Store prediction\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        torch.cuda.empty_cache()  # Manage GPU memory for long runs\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pddlvm)",
   "language": "python",
   "name": "pddlvm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
